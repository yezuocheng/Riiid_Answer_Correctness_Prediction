{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import cpu_count\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "submit_kaggle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 s, sys: 3.8 s, total: 14.8 s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cols_to_load = ['user_id', 'answered_correctly', 'content_id', 'prior_question_had_explanation', 'prior_question_elapsed_time']\n",
    "FEATURES = pd.read_pickle('../input/riiid-train-data-multiple-formats/riiid_train.pkl.gzip')[cols_to_load]\n",
    "FEATURES['prior_question_had_explanation'] = FEATURES['prior_question_had_explanation'].astype('boolean')\n",
    "# questions = pd.read_csv('../input/riiid-test-answer-prediction/questions.csv')\n",
    "# lectures = pd.read_csv('../input/riiid-test-answer-prediction/lectures.csv')\n",
    "dtype={'num_user_answered_questions':np.int16, \n",
    "       'num_user_correctly_questions':np.int16, \n",
    "       'mean_user_accuracy':np.float32,\n",
    "       'watches_lecture':'boolean',\n",
    "       'part':np.int8,\n",
    "       'content_questions':np.int32,\n",
    "       'mean_content_accuracy':np.float32,\n",
    "       'mean_prior_question_had_explanation':np.float32,\n",
    "       'mean_prior_question_elapsed_time':np.float32,\n",
    "      }\n",
    "user_df = pd.read_csv('../input/data_io/user_df.csv', index_col=0,dtype=dtype)\n",
    "content_df = pd.read_csv('../input/data_io/content_df.csv', index_col=0,dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = FEATURES[FEATURES['answered_correctly'] != -1]\n",
    "FEATURES['answered_user'] = FEATURES[['user_id','content_id']].groupby('user_id').cumcount().astype(np.uint16)\n",
    "FEATURES['answered_correctly_user'] = FEATURES[['answered_correctly', 'user_id']].groupby('user_id').cumsum().astype(np.uint16)\n",
    "FEATURES['answered_correctly_user'] = FEATURES[['answered_correctly_user', 'user_id']].groupby('user_id').shift(1)\n",
    "FEATURES['answered_correctly_user'] = FEATURES['answered_correctly_user'].fillna(0)\n",
    "FEATURES['answered_correctly_user'] = FEATURES['answered_correctly_user'].astype(np.uint16)\n",
    "FEATURES['attempt'] = FEATURES.groupby(['user_id', 'content_id']).content_id.transform('cumcount').astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(FEATURES):\n",
    "    # merge with all features\n",
    "    FEATURES = FEATURES[FEATURES['answered_correctly'] != -1]\n",
    "    FEATURES['answered_user'] = FEATURES[['user_id','content_id']].groupby('user_id').cumcount().astype(np.uint16)\n",
    "    FEATURES['answered_correctly_user'] = FEATURES[['answered_correctly', 'user_id']].groupby('user_id').cumsum().astype(np.uint16)\n",
    "    FEATURES['answered_correctly_user'] = FEATURES[['answered_correctly_user', 'user_id']].groupby('user_id').shift(1)\n",
    "    FEATURES['answered_correctly_user'] = FEATURES['answered_correctly_user'].fillna(0)\n",
    "    FEATURES['answered_correctly_user'] = FEATURES['answered_correctly_user'].astype(np.uint16)\n",
    "    FEATURES = FEATURES.merge(user_df, how=\"left\", on=\"user_id\")\n",
    "    FEATURES = FEATURES.merge(content_df, how='left', on='content_id')\n",
    "    # add harmonic mean\n",
    "    FEATURES['hmean_user_content_accuracy'] = 2 * (\n",
    "        (FEATURES['mean_user_accuracy'] * FEATURES['mean_content_accuracy']) /\n",
    "        (FEATURES['mean_user_accuracy'] + FEATURES['mean_content_accuracy'])\n",
    "    )\n",
    "    for c in categorical_feature:\n",
    "        FEATURES[c] = FEATURES[c].astype('category')\n",
    "    return FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature = [\n",
    "                     'part',\n",
    "                     ]\n",
    "features =['prior_question_elapsed_time',\n",
    " 'mean_user_accuracy',\n",
    " 'answered_correctly_user',\n",
    " 'answered_user',\n",
    " 'mean_content_accuracy',\n",
    " 'part',\n",
    " 'hmean_user_content_accuracy',\n",
    " 'attempt']\n",
    "target = 'answered_correctly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = make_features(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = features + [target]\n",
    "FEATURES = FEATURES[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 579 ms, sys: 192 ms, total: 771 ms\n",
      "Wall time: 769 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if submit_kaggle:\n",
    "    lgb_train = lgb.Dataset(FEATURES[features], FEATURES[target], categorical_feature = categorical_feature)\n",
    "    del FEATURES\n",
    "    gc.collect()\n",
    "else:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(FEATURES[features], FEATURES[target], test_size=0.2, random_state=1)\n",
    "    del FEATURES\n",
    "    gc.collect()\n",
    "    lgb_train = lgb.Dataset(X_train, y_train, categorical_feature = categorical_feature)\n",
    "    lgb_eval = lgb.Dataset(X_valid, y_valid, categorical_feature = categorical_feature)\n",
    "    del X_train, y_train, X_valid, y_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective': 'binary',\n",
    "          'metric': 'auc',\n",
    "          'seed': 1,\n",
    "          'learning_rate': 0.1, #default\n",
    "          \"boosting_type\": \"gbdt\" #default\n",
    "         }\n",
    "METRICS = ['auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-d36ed1b2e82e>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-d36ed1b2e82e>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    verbose_eval = 50,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "evals_result = {}\n",
    "if submit_kaggle:\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        valid_sets = lgb_train,\n",
    "        verbose_eval = 50,\n",
    "        num_boost_round = 300,\n",
    "        early_stopping_rounds = 10,\n",
    "        categorical_feature = categorical_feature,\n",
    "        feature_name = features,\n",
    "        evals_result = evals_result,\n",
    "    )\n",
    "else:\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        valid_sets = [lgb_train,lgb_eval]\n",
    "        verbose_eval = 50,\n",
    "        num_boost_round = 300,\n",
    "        early_stopping_rounds = 10,\n",
    "        categorical_feature = categorical_feature,\n",
    "        feature_name = features,\n",
    "        evals_result = evals_result,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(evals_result):\n",
    "    for metric in METRICS:\n",
    "        plt.figure(figsize=(20,8))\n",
    "        \n",
    "        for key in evals_result.keys():\n",
    "            history_len = len(evals_result.get(key)[metric])\n",
    "            history = evals_result.get(key)[metric]\n",
    "            x_axis = np.arange(1, history_len + 1)\n",
    "            plt.plot(x_axis, history, label=key)\n",
    "        \n",
    "        x_ticks = list(filter(lambda e: (e % (history_len // 100 * 10) == 0) or e == 1, x_axis))\n",
    "        plt.xticks(x_ticks, fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "\n",
    "        plt.title(f'{metric.upper()} History of training', fontsize=18);\n",
    "        plt.xlabel('EPOCH', fontsize=16)\n",
    "        plt.ylabel(metric.upper(), fontsize=16)\n",
    "        \n",
    "        if metric in ['auc']:\n",
    "            plt.legend(loc='upper left', fontsize=14)\n",
    "        else:\n",
    "            plt.legend(loc='upper right', fontsize=14)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "plot_history(evals_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, '../input/data_io/state1_2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = joblib.load('../input/data_io/model_Riiid_Competition_Baseline_1.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
