{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import cpu_count\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "submit_kaggle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 s, sys: 4.14 s, total: 19.7 s\n",
      "Wall time: 24.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cols_to_load = ['user_id', 'answered_correctly', 'content_id', 'prior_question_had_explanation', 'prior_question_elapsed_time']\n",
    "FEATURES = pd.read_pickle('../input/riiid-train-data-multiple-formats/riiid_train.pkl.gzip')[cols_to_load]\n",
    "FEATURES['prior_question_had_explanation'] = FEATURES['prior_question_had_explanation'].astype('boolean')\n",
    "\n",
    "# questions = pd.read_csv('../input/riiid-test-answer-prediction/questions.csv')\n",
    "# lectures = pd.read_csv('../input/riiid-test-answer-prediction/lectures.csv')\n",
    "dtype={'num_user_answered_questions':np.int16, \n",
    "       'num_user_correctly_questions':np.int16, \n",
    "       'mean_user_accuracy':np.float32,\n",
    "       'watches_lecture':'boolean',\n",
    "       'part':np.int8,\n",
    "       'content_questions':np.int32,\n",
    "       'mean_content_accuracy':np.float32,\n",
    "       'mean_prior_question_had_explanation':np.float32,\n",
    "       'mean_prior_question_elapsed_time':np.float32,\n",
    "       'mean_user_question_elapsed_time':np.float32,\n",
    "       'mean_user_question_had_explanation':np.float32\n",
    "      }\n",
    "user_df = pd.read_csv('../input/data_io/user_df.csv', index_col=0,dtype=dtype)\n",
    "content_df = pd.read_csv('../input/data_io/content_df.csv', index_col=0,dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.2 s, sys: 8.61 s, total: 59.8 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FEATURES = FEATURES[FEATURES['answered_correctly'] != -1]\n",
    "FEATURES['answered_user'] = FEATURES[['user_id','content_id']].groupby('user_id').cumcount().astype(np.uint16)\n",
    "FEATURES['answered_correctly_user'] = FEATURES[['answered_correctly', 'user_id']].groupby('user_id').cumsum().astype(np.uint16)\n",
    "FEATURES['answered_correctly_user'] = FEATURES[['answered_correctly_user', 'user_id']].groupby('user_id').shift(1)\n",
    "FEATURES['answered_correctly_user'] = FEATURES['answered_correctly_user'].fillna(0)\n",
    "FEATURES['answered_correctly_user'] = FEATURES['answered_correctly_user'].astype(np.uint16)\n",
    "FEATURES['attempt'] = FEATURES.groupby(['user_id', 'content_id']).content_id.transform('cumcount').astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(FEATURES):\n",
    "    # merge with all features\n",
    "    FEATURES = FEATURES.merge(user_df, how=\"left\", on=\"user_id\")\n",
    "    FEATURES = FEATURES.merge(content_df, how='left', on='content_id')\n",
    "    # add harmonic mean\n",
    "    FEATURES['hmean_user_content_accuracy'] = 2 * (\n",
    "        (FEATURES['mean_user_accuracy'] * FEATURES['mean_content_accuracy']) /\n",
    "        (FEATURES['mean_user_accuracy'] + FEATURES['mean_content_accuracy'])\n",
    "    )\n",
    "    FEATURES['question_elapsed_time'] = FEATURES['prior_question_elapsed_time'].shift(-1)\n",
    "    FEATURES['question_elapsed_time'] = FEATURES['question_elapsed_time'].fillna(FEATURES['mean_user_question_elapsed_time'])\n",
    "    FEATURES['question_elapsed_standard'] = (FEATURES['question_elapsed_time']/FEATURES['mean_prior_question_elapsed_time']).astype('float32')\n",
    "    for c in categorical_feature:\n",
    "        FEATURES[c] = FEATURES[c].astype('category')\n",
    "    return FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.2 s, sys: 10 s, total: 48.2 s\n",
      "Wall time: 50.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mean_user_accuracy              float32\n",
       "answered_correctly_user          uint16\n",
       "answered_user                    uint16\n",
       "mean_content_accuracy           float32\n",
       "part                           category\n",
       "question_elapsed_standard       float32\n",
       "hmean_user_content_accuracy     float32\n",
       "attempt                           uint8\n",
       "answered_correctly                 int8\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "categorical_feature = [\n",
    "                     'part',\n",
    "                     ]\n",
    "features =[\n",
    " 'mean_user_accuracy',\n",
    " 'answered_correctly_user',\n",
    " 'answered_user',\n",
    " 'mean_content_accuracy',\n",
    " 'part',\n",
    " 'question_elapsed_standard',\n",
    " 'hmean_user_content_accuracy',\n",
    " 'attempt']\n",
    "target = 'answered_correctly'\n",
    "\n",
    "FEATURES = make_features(FEATURES)\n",
    "col = features + [target]\n",
    "FEATURES = FEATURES[col]\n",
    "FEATURES.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 956 ms, sys: 232 ms, total: 1.19 s\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if submit_kaggle:\n",
    "    lgb_train = lgb.Dataset(FEATURES[features], FEATURES[target], categorical_feature = categorical_feature)\n",
    "    del FEATURES\n",
    "    gc.collect()\n",
    "else:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(FEATURES[features], FEATURES[target], test_size=0.2, random_state=1)\n",
    "    del FEATURES\n",
    "    gc.collect()\n",
    "    lgb_train = lgb.Dataset(X_train, y_train, categorical_feature = categorical_feature)\n",
    "    lgb_eval = lgb.Dataset(X_valid, y_valid, categorical_feature = categorical_feature)\n",
    "    del X_train, y_train, X_valid, y_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective': 'binary',\n",
    "          'metric': 'auc',\n",
    "          'seed': 1,\n",
    "          'learning_rate': 0.1, #default\n",
    "          \"boosting_type\": \"gbdt\" #default\n",
    "         }\n",
    "METRICS = ['auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 65244627, number of negative: 34026673\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.594159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1559\n",
      "[LightGBM] [Info] Number of data points in the train set: 99271300, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657236 -> initscore=0.650999\n",
      "[LightGBM] [Info] Start training from score 0.650999\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    }
   ],
   "source": [
    "evals_result = {}\n",
    "if submit_kaggle:\n",
    "    model = lgb.train(\n",
    "        params = params, \n",
    "        train_set = lgb_train,\n",
    "        valid_sets = lgb_train,\n",
    "        verbose_eval = 50,\n",
    "        num_boost_round = 300,\n",
    "        early_stopping_rounds = 10,\n",
    "        categorical_feature = categorical_feature,\n",
    "        feature_name = features,\n",
    "        evals_result = evals_result,\n",
    "    )\n",
    "else:\n",
    "    model = lgb.train(\n",
    "        params = params, \n",
    "        train_set = lgb_train,\n",
    "        valid_sets = [lgb_train,lgb_eval]\n",
    "        verbose_eval = 50,\n",
    "        num_boost_round = 300,\n",
    "        early_stopping_rounds = 10,\n",
    "        categorical_feature = categorical_feature,\n",
    "        feature_name = features,\n",
    "        evals_result = evals_result,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(evals_result):\n",
    "    for metric in METRICS:\n",
    "        plt.figure(figsize=(20,8))\n",
    "        \n",
    "        for key in evals_result.keys():\n",
    "            history_len = len(evals_result.get(key)[metric])\n",
    "            history = evals_result.get(key)[metric]\n",
    "            x_axis = np.arange(1, history_len + 1)\n",
    "            plt.plot(x_axis, history, label=key)\n",
    "        \n",
    "        x_ticks = list(filter(lambda e: (e % (history_len // 100 * 10) == 0) or e == 1, x_axis))\n",
    "        plt.xticks(x_ticks, fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "\n",
    "        plt.title(f'{metric.upper()} History of training', fontsize=18);\n",
    "        plt.xlabel('EPOCH', fontsize=16)\n",
    "        plt.ylabel(metric.upper(), fontsize=16)\n",
    "        \n",
    "        if metric in ['auc']:\n",
    "            plt.legend(loc='upper left', fontsize=14)\n",
    "        else:\n",
    "            plt.legend(loc='upper right', fontsize=14)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "plot_history(evals_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, '../input/data_io/state2_2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = joblib.load('../input/data_io/model_Riiid_Competition_Baseline_1.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
